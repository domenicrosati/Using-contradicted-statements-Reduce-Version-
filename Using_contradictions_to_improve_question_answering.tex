% docx2tex 1.7.3 --- ``Just out of this Word.'' 
% 
% docx2tex is Open Source and  
% you can download it on GitHub: 
% https://github.com/transpect/docx2tex 
%  
\documentclass{scrbook} 
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc} 
\usepackage{hyperref} 
\usepackage{multirow} 
\usepackage{tabularx} 
\usepackage{color} 
\usepackage{textcomp} 
\usepackage{tipa}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{amsfonts} 
\usepackage{amsxtra} 
\usepackage{wasysym} 
\usepackage{isomath} 
\usepackage{mathtools} 
\usepackage{txfonts} 
\usepackage{upgreek} 
\usepackage{enumerate} 
\usepackage{tensor} 
\usepackage{pifont} 
\usepackage{ulem} 
\usepackage{xfrac} 
\usepackage{soul}
\usepackage{arydshln} 
\usepackage[english]{babel}
\definecolor{white}{rgb}{1,1,1}
\definecolor{color-FFFFFF}{rgb}{1,1,1}
\definecolor{color-434343}{rgb}{0.26,0.26,0.26}

\begin{document}
Retrieval-guided Counterfactual Generation for QANeed more hereIntroduced by KamathKnow What You Don’t Know: Unanswerable Questions for SQuADRajpurkar, Jia, Liang 2018Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)Doesn't include hallucainationsFind citation with hallucincationsExpand and work in Abcha and HicksI don't think thats quite right.Using contradicted statements for improving QA systemsAbstractEnsuring the safety and interpretability of question answering (QA) systems is critical for deploying them in contexts such as biomedical and scientific domains. One dominant approach to improving these systems uses natural language inference (NLI) to determine whether answers are entailed by some background context and thereby verifying the answer. Our work challenges this paradigm by looking at the dynamics of contradiction and we propose a contradiction-based system for choosing answers for multiple choice and extractive QA. While the contradiction-based system is competitive with and sometimes better than entailment only systems, we show that a calibration model that incorporates contradiction, entailment, and QA model confidence scores outperforms other systems in all cases. Based on this result, we explore how question answering systems might leverage NLI classifications in general. Finally, we discuss the implications of this result to explore the unique opportunities for systems using contradiction on a wider scope of question answering such as improving interpretability and safety in open domain settings.IntroductionSafety in NLP systems is a major unresolved issue, particularly when deploying these systems in biomedical and scientific contexts where known issues such as hallucination and overconfidence provide major obstacles to deploying them (Kell et al., 2021). Utilizing natural language inference (NLI) as a method for improving the safety and performance of NLP research is an active area of research (Laban et al., 2022). However, these systems typically focus on utilizing the entailment signal to verify answers. Similar research looks at building self-supporting NLP systems (Nakano et al., 2022; Menick et al., 2022) with the goal of improving safety by verifying its outputs with some external source. We find these developments troubling since the critical rationalist philosophy of science (Miller, 2015) would predict that verification-based systems are not possible even in principle because statements can only be falsified with critical tests and never verified. Inspired by that prediction, we would like to explore refutation-based systems. In particular we look at an open research question: Using an NLI-based system how do contradictions contribute to the performance and safety of QA and how is this different from entailment or ``verification'' based systems. By exploring this question below, we hope to show why researchers should be critical of the paradigm of verification in NLP systems and why future work utilizing critical and contradicted statements could improve the safety of NLP systems.Related WorkTextual Entailment for QANLI is used widely for a variety of downstream NLP tasks (Yin et al., 2020). Much of this work has been to use NLI to improve QA systems (Paramasivam and Nirmala, 2021) and have shown some impressive performance gains in multiple choice (Mishra et al., 2021), extractive QA (Chen et al., 2021), and Multihop QA (Trivedi et al., 2019) settings. However, despite showing improvements by using NLI, these approaches have thus far focused on using entailment as a verification mechanism. Chen finds that under selective question answering (Kamath et al., 2020) for extractive QA, NLI systems can verify QA systems’ predictions. Contrastive, Counterfactual, and Contradicted StatementsCounterfactual, contrastive, and contradicted statements are similar in that they provide statements which differ in some way from some original or expected statement. However, each is different in the context in which they are used. For counterfactual statements in a QA context, we ask what effect does a change in a context or question have on the answer, or what change in context or question is needed for a counterfactually perturbed answer (Kaushik et al., 2020). Counterfactual statements are typically used to improve NLP systems by either augmenting datasets such that spurious correlations are avoided and better generalization is achieved or as a method for probing biases or performance gaps in already trained models. Contrastive statements on the other hand are designed to act as a negative sample in order to improve the representations learned during model training through contrastive learning (Schuster et al., 2021; Cao and Wang, 2021). Both areas are active and have demonstrated impressive results by utilizing counterfactuals or contrastive statements for training and evaluation (SEE: list three papers) . With contradicted statements, our primary concern is to have some statement that contradicts another statement without the need for it to be a perturbation of some original statement like a counterfactual or provide contrast to some positive sample. (Cite?)Evaluating NLP Models via Contrast SetsFactual ConsistencyThe only work we are aware of that leverage contradictions are within the domain of factually consistency, which seeks to ensure that a collection of utterances don't contain any contradictions. In the case of abstractive summarization, many works are concerned with improving their factuality and reducing hallucinations (Li et al., 2022) and even use NLI quite widely like the QA task but mostly are focused on entailment.. Laban et al. (2022) propose a NLI based method to ensure the consistency of a summary with a source document that achieves SOTA results. Importantly, they show that by incorporating contradiction scores with entailment they achieve the best performance. Similarly QAFactEval (Fabbri et al., 2022) improves on Laban et al. (2022) and incorporates all NLI class scores not simply entailment. Finally, contradiction detection has surfaced as an important tool in generating dialogues that are consistent with a persona (Nie et al., 2021; Song et al., 2020). Hsu et al., (2021) show an interesting use case of leveraging contradictions to prune self-contradictions within wikipedia documents.ContributionsOur work provides the following contributions:We propose a method for being able to incorporate the contradiction signal in answer ranking for multiple choice and extractive question answering set upsWe show that a fully calibrated model that uses QA confidence scores as well as both entailment and contradiction score outperforms other setups and that a model calibrated with contradictions only is competitive with and even better than entailment only models in some settingsBased on the results presented below we discuss how future work could take advantages of the unique opportunities that contradicted statements offer.MethodOverviewWe take a similar approach to Chen et al. (2021) and Mishra et al. (2021) where we turn question answer pairs into declarative statements (QA2D) using a question answer to declarative statement model (Demszky et al., 2018) and then use a NLI model with a context passage as the premise and the declarative statement as a hypothesis. While the answers are provided for the multiple choice setting, we use an extratice QA model to produce answers for the MRQA datasets. We also use QA models for each setting to produce confidence scores.Once we have the NLI three class classification (entailment, neutral, contradiction), we use the confidence for the classification as a ranking signal for ranking answers. In the extractive QA case, we rank questions for selective question answering (Kamath et al., 2020) where we rank a single answer for each question by how confident we are in answering that question. In the multiple choice case, we use the NLI classification of each answer to determine which multiple choice answer to select. Like Chen et al. (2021) we use a calibration method that includes the confidence scores from the QA models and the NLI models.QA ModelsFor the multiple choice setting, we wanted to test how the proposed architecture performs under domain shifts so we used roberta large ((Liu et al., 2019)) fine tuned on the race dataset as well as a two deberta v3 (He et al., 2021) variants (base and xsmall) on SciQ. For the extractive QA setting, we used DistillBERT and BERT Large models trained on SQuAD. As well as used the baseline models from the MRQA 2019 task (Fisch et al., 2019). Training and validation details are available in Appendix A.QA2DA QA2D model reformulates a question-answer pair to a declarative statement Demszky et al., 2018). As noted in Chen et al. (2021) and Mishra et al. (2021), the QA2D reformulation is critical to using NLI models in a question answering task in order to have the proposed answer match the format of NLI which requires declarative statements as hypothesis. More formally, we would like to transform a question q answer q pair (q, a) to a truth conditional proposition P such that P is found to be entailed (C${\vdash}$P), contradicted (C${\bot}$P), or neither entailed nor contradicted (C or P) by some other context statement C. In order to do this we trained a series of T5 models (Raffel et al., 2020) on the dataset proposed by Demszky et al. (2018). We found almost no noticeable differences in performance across models and opted for usage of the T5 small model in our pipeline. Unlike Chen et al. (2021), we found that regardless of size, these QA2D models struggled with long questions or questions with complex syntax and would often leave the answer out of the statement. In order to solve for this, we tried using constrained decoding that required the answer to be in the statement - however this often produced ungrammatical or nonsensical statements. We settled with the following heuristic to postprocess QA2D outputs. If less than 50\% of the tokens in the answer were in the statement then we appended the answer to the end of the statement. We used 50\% to account for rephrasing the answer or swapping pronouns. While some statements resulted in answer redundancy, we felt that this was better than having hypotheses which left out the answer statement. Table 2 provides examples of the errors discussed above. Future work should focus on how we can use these models outside of the domains in the dataset provided by Demsky et al. (2018).NLISince we’d like to know whether the reformulated answer statement is contradicted, entailed, or neutral w.r.t to the context passage as part of our architecture, we used a series of NLI models to test each context passage against the reformulated answer statements. Specifically we used two deberta-based models (He et al., 2021) trained on the MNLI dataset and an albert model (Nie et al., 2020) trained on the ANLI dataset in addition to various other NLI datasets. Using these models we performed NLI on the context passages as the premise and the reformulated answer statement as the hypothesis for each answer in both the multiple choice and extractive QA settings.Calibration ModelsIn order to make use of the NLI models and QA confidence scores together, we developed a set of calibration models that use logistic regression to predict whether the selected answer will be correct based on a linear combination of confidence scores from the QA model and each NLI classification score. Table 3 below outlines the regression analysis associated with each calibration model used.While we use a linear combination of contradiction and entailment, other features could be used.Answer RankingMention this is the same as hicks but instead of using just entailment we can use entail. Contra, calibration.Finally, in order to use the calibration models or the NLI scores we have developed the following setups. For the multiple choice setting, we have NLI only ranking which will rank the questions based on the entailment score or the inverse contradiction score (selecting the least contradicted answer) of the reformulated answer statement. We also test ranking by calibration models which use the probability score assigned to the likelihood of the answer being correct. While we could use this set up for extractive QA, we would need models that produce diverse answer sets. We found that using a topk=4 approach to extractive QA generated almost the same answers with slightly different spans so we did not use the approach with extractive QA.For both the multiple choice and extractive QA settings we ranked answers selected by the QA model by how confident each calibration or NLI only configuration was at answering the question. We used these confidence scores to select a top n percentage of questions to answer so we could compare each approach at difference coverage thresholds.Please note that for each of these setup’s we are using a three class NLI model, therefore contradiction is not simply the inverse of entailment as it might by in the two class setup that is common with approaches using data augmentation to produce in-domain NLI datasets (Mishra et al., 2021).DatasetsFor both settings, we used datasets where the context passage the answer is found in is already available. We also wanted to be sure we selected a wide variety of datasets for each setting so we could test the viability of our approach. The multiple choice setting we used a comprehensive set of 9 datasets. For the extractive QA case we used 6 of the datasets from the MRQA 2019 task (CITE) as well as SQuAD2.0 (CITE) and SQuAD (CITE) adversarial for a total of 8 extractive QA datasets. Each setting has in-domain as well as out of domain datasets for testing on. See appendix XXX for more details on the datasets used.Note validationResultsRanking multiple choice answers by EntailmentIn tables 6 (NLI only) and 7 (calibrated) we present the accuracy achieved on each of the 9 datasets. We show results with each QA model and the deberta v2 xxlarge model trained on MNLI. Less performant models such as the ANLI and deberta base model are presented in the appendix but highlight similar trends discussed below. On the NLI-only results presented in table 6, the baseline QA model on race outperforms on most datasets except mctest and the combination of entailment and contradiction tends to do second best with the exception of CosmosQA where the least contradiction rank does best and second best and MCScript where entailment does best. Notably, the SciQ models do much worse than the NLI only ranking for either class except on in-domain questions for SciQ and the similar QASC dataset. These results show that an NLI-only approach can be competitive with a robust QA model and better than a QA model trained in a specific domain. Notably, we also show that incorporating the contradiction scores with the entailment scores is better than entailment alone and that selecting the least contradicted answer is quite competitive with selecting the most entailed answer. Additionally, other than the QA model, the contradiction model has the lowest variance of 2.65\% indicating that selecting the least contradicted answer may help with more stable performance actress domains.When we look at the results from the calibration models we see that the calibrated models outperform the QA models in all cases in-domain with RACE where they perform the same. The best calibration incorporates the QA confidence, entailment, and contradiction achieving an average accuracy of 84.57\% over 84.09\% achieved by the RoBERTa-large model trained on RACE. The second best approach is the calibration with contradiction only (84.33\%), however only slightly over entailment only (84.32\%). Although only slightly less than the QA model (1.4\%) the calibration with contradiction only achieves the lowest variance of 1.3\% lending support to the finding above that selecting the least contradicted answer may help with generalization.modelcosmosdreamMCScriptMCScript2mctestqascracerace\_csciqavgvarsciq-base18.4643.8061.9963.7144.7693.4130.9727.3995.2853.317.63sciq-small25.4648.2660.2866.0459.7690.6035.5630.6298.0957.196.43raceQA64.2282.5689.7086.9890.4898.1676.9369.8097.9684.091.40mnli-largeE + C44.3680.9485.5284.9990.6096.4464.2951.4092.4776.783.54E36.1879.0386.0279.7289.8895.9062.1449.7291.9674.514.27C59.2678.9883.1284.4389.2992.7662.7447.0591.5876.582.65Table \textasciicircum{}: Accuracy scores on NLI only answer ranking. The best scores are bold and the second best are underlined.modelcosmosdreamMCScriptMCScript2mctestqascracerace\_csciqavgvarsciq-base18.4643.8061.9963.7144.7693.4130.9727.3995.2853.317.63sciq-small25.4648.2660.2866.0459.7690.6035.5630.6298.0957.196.43raceQA64.2282.5689.7086.9890.4898.1676.9369.8097.9684.091.40mnli-largeQA + E + C64.7283.1990.0687.5991.4398.6077.5369.8098.2184.571.42QA + E64.3282.8589.9287.2991.0798.4977.1869.6698.0984.321.43QA + C64.8282.7589.8887.2990.8398.3877.1669.8098.0984.331.39Table 6: Accuracy scores on calibrated NLI answer ranking. Calibrations are with the Roberta large model trained on the race dataset. The best scores are bold and the second best are underlined.Selective QA``'''' validating answers to unanswerable questions (Rajpurkar et al., 2018; Kwiatkowski et al., 2019), but such questions may be nonsensical in context; these efforts do not address whether all aspects of a question have been covered. Methods to handle adversarial SQuAD examples (Jia and Liang, 2017) attempt to do this (Chen and Durrett, 2021), but these are again geared towards detecting specific kinds of mismatches between examples and contexts, like a changed modifier of a noun phrase. Kamath et al. (2020) frame their selective question answering techniques in terms of spotting out-of-domain questions that the model is likely to get wrong rather than more general confidence estimation. What is missing in these threads of literature is a formal criterion like entailment: when is an answer truly sufficient and when are we confident that it addresses the question?‘’’ chenFor the selective QA evaluation, the QA model selects the answer and then we evaluate the top 20\% or 50\% of those answers after sorting them by confidence score, NLI score, or the probability predicted by a calibration model that the model is likely to be correct. In the multiple choice setting we do not select the answers with the ranking as proposed in the method above since the results were not as good (see Appendix B). Selective QA in the Multiple Choice SettingIn table 7, the first thing to note is that the QA model calibrated with contradiction scores only performs the best on selective QA, achieving best or second best on almost every dataset for 97.57\% at 20\% coverage and 94.74\% at 50\% coverage over 97.45\% at 20\% coverage and 94.52 at 50\% coverage achieved by QA model. This is especially striking at 50\% coverage where the QA model only does significantly better on the in domain race datasets. The contradiction calibration model is also the only model to outperform the QA model ranking by confidence score. While the NLI only models can be competitive with the calibrated models, we also highlight that sorting by the least contradicted achieves quite good performance and is often quite a bit better, 94.79\% at 20\% coverage and 92.13\% at 50\% coverage, than sorting by the most entailed (93.50\% / 91.24\%) or a combination of entailed and contradicted (93.55\% / 91.89\%). These results would be inline with our intuition that the least contradicted an answer is the more likely it is correct even in cases where we might be uncertain about its entailment.dataset\_nameQA + E + CQA + CQA + EE + CECQAACC @ 20\%cosmos\_qa77.5591.1276.8869.1868.3483.2588.61dream98.2898.7798.2896.3296.3296.8198.28MCScript99.8299.4699.8299.6499.6499.4699.82MCScript-2.099.5899.7299.4599.1799.0397.3799.58mctest100.0099.40100.00100.00100.0099.4098.81qasc100.00100.00100.00100.00100.00100.00100.00race94.9396.6994.7292.4492.2490.1798.24race\_c88.7392.9689.4485.2185.9286.6293.66sciq100.00100.00100.00100.00100.00100.00100.00avg95.4397.5795.4093.5593.5094.7997.45ACC @ 50\%cosmos\_qa80.2981.7076.9475.8070.6480.6376.47dream95.1096.8694.9093.6393.6393.6396.67MCScript98.5798.6498.2898.0097.9397.1498.78MCScript-2.096.4098.2395.8494.6894.4096.0198.01mctest99.5299.7699.5299.0599.0599.7699.52qasc100.00100.00100.0099.7899.7899.78100.00race90.1192.6889.9987.7187.3885.2393.88race\_c85.1184.8385.3978.3778.3777.2587.36sciq100.00100.00100.00100.00100.0099.74100.00avg93.9094.7493.4391.8991.2492.1394.52Table 7SelectiveQA in the extractive QA settingsFor the extractive QA setting we present the same analysis in table 8. We do observe similar trends as above where calibration with contradiction only does quite a bit better than the QA model (74.19\% vs 72.77\% at 20\%, 68.46\% vs 67.76\% at 50\%) and that of the NLI only ranking selecting the least contradicted does best. Although only slightly better than the contradicting only, the results show that the QA + E model does best at 20\% coverage and the QA + E + C model does best at 50\% coverage. This indicates that entailment is still an important signal, albeit more powerful when combined with contradiction. Appendix B contains a comparison with a smaller model which shows similar results.datasetQA + E + CQA + EQA + CE + CECQAF1 @ 20BioASQ85.0485.0683.1074.2274.2275.4782.99HotpotQA86.6286.6985.8980.6080.6079.8285.33NaturalQuestions91.8491.6892.1879.8979.8782.0990.98SQuAD98.2698.7698.1792.3792.4890.8899.04squad\_adversarial43.9943.9843.5743.7443.6042.8139.83squadv237.6437.5636.0737.4337.3137.6830.52TriviaQA81.3381.2180.3665.5365.2569.1380.68avg74.9674.9974.1967.6867.6268.2772.77F1 @ 50BioASQ76.1376.0475.5171.4971.4972.9775.49HotpotQA79.3779.3078.9577.4377.4377.3178.74NaturalQuestions84.5384.4883.2474.9674.9378.6282.47SQuAD96.9896.9797.0191.5891.5291.1997.00squad\_adversarial41.8041.1641.4942.7642.7942.0340.26squadv229.4128.4528.7734.4334.1434.3926.18TriviaQA74.3074.3774.2365.0564.9368.0874.21avg68.9368.6868.4665.3965.3266.3767.76Table 8: Selective QA Answer Rejection on SQuAD 2.0In order to explore how useful contradiction might be over other setups, we evaluated answer rejection in SQuAD 2.0 using the BERT-large model. This setting evaluates how well a model does at abstaining from answering a question when there is no answer. We selected three setups, rejecting answers by QA confidence, by entailment score, and by contradiction score. Notably, when we select by least entailed answers we end up treating the problem as two class NLI (entailed v not entailed) which has been used previously (Chen et al., 2021).Table 9 shows that the NLI based setups outperform QA confidence setups. In particular the difference between rejecting answers that are not entailed and rejecting answers that have been contradicted appears to be a precision versus recall tradeoff. The overall model (best f1 score) is achieved by rejecting answers where the contradiction score was greater than 5\% successfully rejecting 76.15\% answers and accepting 93.23\% answers. Rejecting answers if they are not entailed, where E {\textless} 50\%, achieves the second best F1 score and illustrates an interesting dynamic. E {\textless} 50\% has the best recall (38.52\%) while C {\textgreater} 50\% has the best precision (97.06\%). This result shows that if we want to build systems that err on the side of rejecting correct answers then selecting answers that are not entailed has an advantage. Conversely, if we want to build systems that are better at rejecting only answers that should be rejected then rejecting contradicted answers is a better strategy. Interestingly our results highlight what has been discussed above: the utility of using contradiction confidence scores even if they are low. In the context of safety critical settings, where we want to reject the most answers that might cause harm by falsely answering them, we hope to have shown that a contradiction confidence based approach to abstention would be useful.tpfprejectsacceptsprecisionrecallf1QA {\textless} 50\%2777164446.71\%86.15\%62.81\%23.39\%34.09\%QA {\textless} 25\%132554022.29\%95.45\%71.05\%11.16\%19.29\%QA {\textless} 75\%4234322271.22\%72.86\%56.79\%35.66\%43.81\%E {\textless} 5\%260415043.80\%98.74\%94.55\%21.93\%35.61\%E {\textless} 25\%379440663.82\%96.58\%90.33\%31.95\%47.21\%E {\textless} 10\%310023552.14\%98.02\%92.95\%26.11\%40.77\%E {\textless} 50\%4574100776.94\%91.52\%81.96\%38.52\%52.41\%C {\textgreater} 50\%25437742.78\%99.35\%97.06\%21.42\%35.09\%C {\textgreater} 25\%322316854.21\%98.59\%95.05\%27.15\%42.23\%C {\textgreater} 10\%397641666.88\%96.50\%90.53\%33.49\%48.89\%C {\textgreater} 5\%452780476.15\%93.23\%84.92\%38.13\%52.63\%Table 9: SelectiveQA: MRQA Total 11873 / Total empty 5945Correlation AnalysisSince we are using the NLI and confidence scores to construct the setup’s above, we’d like to know how confidence and NLI score correlate with the correct answer. Table 10 shows how the NLI classes correlate both by score and by actual classification (score {\textgreater} 50\%) as compared against QA model confidence score. The multiple choice analysis shows answers from the RoBERTa-large race model and the extractive QA analysis shows answers from the BERT-large model trained on SQuAD. The correlation analysis presents spearman rank correlations. What we see below is that in the multiple choice setting the confidence score has a strong correlation with the correct answer which makes sense given the confidence score is a softmax over the multiple choice classes. Extractive QA confidence scores have a much weaker correlation and tend to be less of a correlation than entailment has with the correct answer. Despite the results presented above, contradiction only has a notable correlation with the correct answer when the confidence score is used in the multiple choice setting. This justifies our answer ranking approach. Interestingly in the extractive QA case the neutral class is more negatively correlated when selecting by classification. Our conjecture would be that in the extractive QA case we don’t have much to compare against.contradictionentailmentneutralQAmultiple choiceConfidence-0.470.37-0.060.71Class-0.280.38-0.06extractive QAConfidence-0.160.31-0.120.19Class-0.150.39-0.29Table 10: Correlation analysis.Appendix XXX shows per model? Shows per dataset?DiscussionContradiction as a SignalGiven the results above, we hope to have demonstrated that incorporating contradiction is an important signal for QA systems. Unlike two class NLI systems where the contradiction signal is encapsulated with the neutral signal in not-entailed, three class NLI gives us a unique signal to look at if a hypothesis contradicts a premise. Contradiction is a particularly important signal because it lends interpretability to systems taking advantage of it. If we are choosing answers to based on the least contradicted answer, we have information about the other answers and why we didn’t select them. Namely that they were contradicted. Entailment and QA model confidence do not have the same interpretability since all we know about the other answers is they have a lower entailment or confidence score, they could still be correct or entailed, and we would not know if they were neutral or contradicted with respect to the premise. Once we know an answer is contradicted we can use that information to try retrieving another answer or we can use that answer to modify a prompt to indicate the model should use it as a hint that the answer is not the correct one. Entailment does not lend itself to this iterative refinement of question answering and we suggest that future work on utilizing contradiction should investigate a system of this nature.Contradiction also provides a unique opportunity for open domain QA systems where we need to retrieve a proceeding context. In an open domain setting contradiction could be used to select passages or we could attempt to retrieve passages that would contradict an answer to determine if the proposed answer might be wrong. Finally contradicted statements are already being used in a generative setting to improve fact verification systems (Wright et al., 2022; Pan et al., 2021; Saakyan et al., 2021) but we can take this further in and by attempting to provide some criticism for an answer instead of support. Recent work (Saunders et al., 2022) has show that self-criticism is a powerful technique for improving the quality of summaries in language odel type things.LimitationsDespite the results above, multiple choice QA and extractive QA with a provided context is a limited setting that doesn’t indicate the results would extend to other popular settings where NLI are used such as abstractive QA, summarization, factual consistency and so on. We are hopeful given that Laban et al. (2022) shows the same result that contradiction is an important signal in factual consistency. The primary problems that we envision are outlined below.Context Length and NLI DatasetsEven though there is a greater tendency to use NLI in zero-shot settings (Yin et al., 2020). Domain transfer is a known issue with using NLI models. In particular, NLI datasets tend to focus on textual entailment over short passages such as sentence pairs (Mishra et al., 2021). Even when in-domain datasets are created (Mishra et al., 2021; Khot et al., 2018; Chen et al., 2021). They tend to focus on data augmentation strategies that produce two-class NLI datasets (entail, not entailed) which wouldn’t give us any contradiction signals. Future work should pick up on producing models capable of performing textual entailment over longer passages and devising methods for generating three class NLI datasets so that we can determine if contradictions receive the same benefits from those approaches that entailment has.Our results showing that the SOTA model on ANLI performing poorly (see Appendix X) indicates that we still have much more work to do to improve upon MNLI.Ranking requires alternatives and timeIn the extractive QA setting presented above we cannot use the answer ranking approach we introduced without developing QA models that have more diverse outputs.  Like other textual entailment based systems, this speaks to the computational expense involved in generating and evaluating answer alternatives. If we were to apply our method to an open domain setting where a set of context passages are retrieved, the ranking procedure would require a quadratic evaluating procedure of each context passage against each reformulated answer candidate. In addition, work on retrieval for QA tends to focus on retrieving passages that might support an answer. Our approach implies that a retrieval system that could retrieve critical passages for a given answer might be desirable.SummaryWe have demonstrated that incorporating contradiction is an important signal for multiple choice and extractive QA systems. By proposing a method that reformulates answers as hypothesis statements that can be criticized or critically tested against a premise, we were able to rerank answers and demonstrate that models that are fully calibrated with QA model confidence as well as entailment and contradiction scores outperform QA models by themselves in all cases. In addition, models calibrated with contradiction only or simply selecting the least contradicted answers with NLI only provides a competitive approach to selecting answers that is often on par with or better than entailment only systems. These results show that we should rethink the paradigm of verifying answers with entailment across NLP setups. While this work is in a relatively limited setting, we provide discussion on how leveraging contradictions could help improve open domain and abstractive QA as well as other NLP tasks at large.BibliographyAppendix A: Training SetupIn addition to several pretrained models, we trained 3 models that were used in our setups, a t5-small model on Demszky et al. (2018) for the QA2D set up where a Rogue1 of 90.73\% is achieved on the validation set, deberta-v3 models (xsmall and base) trained on SciQ (Welbl et al. 2017) achieving 93.99\% accuracy on the xsmall model and 91.76\% accuracy on the base model. All models were trained using the huggingface trainer API (Wolf et. al., 2019) with an AdamW optimizer at a learning rate of 5.60e-05 with weight decay of 0.01 more details are available on table XXX below.modeldatasetepochsscoret5-smallDemszky et al. (2018)20Rogue190.73deberta-v3-xsmallWelbl et al. (2017)6Accuracy93.99deberta-v3-baseWelbl et al. (2017)6Accuracy91.79Appendix B: Dataset Detailsdatasetsplitsizecitecosmos\_qavalidation2985Huang et al. (2019)dreamtest2041Sun et al. (2019)MCScripttest2797Ostermann et al. (2018)MCScript-2.0test3610Ostermann et al. (2019)mctesttest840Richardson et al. (2013)qascvalidation926Khot et al. (2020)racetest4934Lai et al. (2017)race\_ctest712Liang et al. (2019)sciqtest884Welbl et al. (2017)Table XX: Multiple choice datasets used.BioASQ1504Fisch et al.,(2019)TriviaQA7785HotpotQA5901SQuAD10506Natural Questions12836SQuAD2.011871Rajpurkar et al. (2018)SQuAD adversarial5347Jia and Liang (2017)Table XX: Extractive QA datasets used. On testAppendix C:Three model comparisonmodelcosmosdreamMCScriptMCScript2mctestqascracerace\_csciqavgvarsciq-base18.4643.8061.9963.7144.7693.4130.9727.3995.2853.317.63sciq-small25.4648.2660.2866.0459.7690.6035.5630.6298.0957.196.43raceQA64.2282.5689.7086.9890.4898.1676.9369.8097.9684.091.40mnli-baseQA + E + C64.3282.6689.6387.0190.7198.2776.9569.8098.0984.161.41QA + E64.2582.6689.6386.9890.7198.2776.9569.8097.9684.141.41QA + C64.2982.5689.6387.0190.6098.1676.9369.8097.9684.101.40E + C33.0362.2776.7672.1168.5792.6645.1634.4188.0163.664.80E27.8162.4779.3771.9468.8192.6643.4834.4188.0163.225.40C43.4559.1970.1869.9767.5081.8641.8132.5887.3761.553.53mnli-largeQA + E + C64.7283.1990.0687.5991.4398.6077.5369.8098.2184.571.42QA + E64.3282.8589.9287.2991.0798.4977.1869.6698.0984.321.43QA + C64.8282.7589.8887.2990.8398.3877.1669.8098.0984.331.39E + C44.3680.9485.5284.9990.6096.4464.2951.4092.4776.783.54E36.1879.0386.0279.7289.8895.9062.1449.7291.9674.514.27C59.2678.9883.1284.4389.2992.7662.7447.0591.5876.582.65anliQA + E + C64.1982.5689.7087.0690.4898.1676.9369.8097.9684.091.41QA + E64.1982.5689.7087.0690.6098.1676.9369.8097.9684.111.41QA + C64.2282.5689.7086.9890.4898.1676.9369.8097.9684.091.40E + C35.7168.2079.5573.8877.5091.7949.0539.4790.8267.334.45E33.6768.3579.9173.1977.3891.9049.0739.1990.9467.074.64C45.1663.7473.5872.7173.3377.8646.3438.2087.2464.242.90With ranking selective QA + With smaller model for etractivedataset\_nameQA + E + CQA + EQA + CE + CECQAACC @ 20\%cosmos\_qa0.780.670.830.200.270.680.89dream0.980.960.970.810.920.940.98MCScript1.001.000.990.930.990.971.00MCScript-2.01.000.990.970.920.970.951.00mctest1.001.000.990.850.970.970.99qasc1.001.001.000.971.000.991.00race0.950.920.900.630.770.750.98race\_c0.890.850.870.710.750.690.94sciq1.001.001.000.821.000.961.00avg0.950.930.950.760.850.880.97ACC @ 50\%cosmos\_qa0.800.710.810.320.350.650.76dream0.950.940.940.850.890.880.97MCScript0.990.980.970.950.960.930.99MCScript-2.00.960.940.960.910.920.920.98mctest1.000.991.000.910.950.961.00qasc1.001.001.000.980.990.981.00race0.900.870.850.680.720.680.94race\_c0.850.780.770.670.670.550.87sciq1.001.001.000.890.960.961.00avg0.940.910.920.800.820.840.95SelectiveQA using distillbertdatasetQA + E + CQA + EQA + CE + CECQAF1 @ 20BioASQ70.9770.4171.5574.0774.0774.3468.99HotpotQA73.4473.0870.8871.5971.5170.4169.41NaturalQuestions85.5985.2985.4578.4678.4680.5383.27SQuAD96.2296.4595.7783.1583.0981.3797.15squad\_adversarial40.3939.7539.4940.0739.5640.5931.98squadv235.4635.2433.6436.3636.1336.6625.95TriviaQA64.9664.6864.5552.6752.0952.5663.98avg66.7266.4165.9062.3462.1362.3562.96F1 @ 50BioASQ65.9665.9264.3763.5363.5366.9564.79HotpotQA64.4264.2163.6565.8865.8566.9162.81NaturalQuestions72.2871.9970.8267.5467.5174.1869.95SQuAD92.5692.5792.3481.8682.2180.9592.54squad\_adversarial33.6932.9033.4538.7438.2238.5231.89squadv226.6825.7026.0032.9532.6132.8323.52TriviaQA58.4058.4158.2551.4351.1852.9958.25avg59.1458.8158.4157.4257.3059.0557.68Appendex DAppendix E: Correlation and RegressionNLISciQRACEcontradictionentailmentneutralallinallinmnli-baseConfidence-0.240.23-0.050.360.750.710.65Class-0.160.22-0.04mnli-largeConfidence-0.470.37-0.060.360.750.710.65Class-0.280.38-0.06anliConfidence-0.320.29-0.170.360.750.710.65Class-0.140.24-0.11Table 2: Correlation AnalysisQA ModelNLI ModelCombinationConfidenceEntailmentContradictionAccSciQmnli-baseQA + Contradiction4.13-1.060.99QA + Entailment3.901.370.99QA + Entailment + Contradiction3.831.22-0.760.99Entailment + Contradiction2.56-1.470.86mnli-largeQA + Contradiction3.98-1.320.99QA + Entailment3.781.550.99QA + Entailment + Contradiction3.651.31-0.970.99Entailment + Contradiction2.63-1.720.91RACEmnli-baseQA + Contradiction3.04-0.150.89QA + Entailment3.030.270.89QA + Entailment + Contradiction3.020.26-0.140.89Entailment + Contradiction0.73-0.460.75mnli-largeQA + Contradiction2.970.00-0.810.89QA + Entailment2.910.980.89QA + Entailment + Contradiction2.850.92-0.750.89Entailment + Contradiction1.76-1.120.78Table 3: RegressionTODO:Draft 1 (Satuday)Fill in methods tablesTable CaptionsMore correlations? No since yuckyProof read and criticise Draft 2 (sunday)Proofread and criticise (Ref check?)Read / Search for relatedDraft 3 (monday)Clean up code for submissionTypesetRemarkable Notes?Tue-WednsdayRead for criticismThrusdayFinal ReadAttempt SubmissionNotesMinimal Pairs!Kaushik 2020Contrasts Sets - generating them for interpertabilityGardner - Contrasts SetsThey end up forming counterfactuals but they are critical statementsNotes to synthesize:Fill in results section firstYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692.NLI Only better than dumb QA models. More general?Discussion: What if critical statements are useful?Entailment based approaches to improve QARegression AnalysisCorrelation Analysis (-Contra score, f1) (entail, f1)Limitations - Open Domain, Context Length, Data Augmentation, Expensive, Extractive - Diverse setsMethods are developed to create in domain entailment / not-entailment binary datasets but none to create three class which would be required       Future Work - Retrieval, generation - critical and diverse, improvements to QA2DNext Steps:Add the MRQA things in an appendixRemaining Experiments:Fix up evaluation models to prepare for when those come inCompile results as I had before and think about what other evaluations do i need to do?Lexical overlap ect.Bibliography and Literature ReviewReread Chen and its associated papersAppendix A per datasetJune 24, 2022Not minididsertation doesnt touch on ssl or generative or factuality. Doesn’t really touch on retrieving. But does provide an important result and roadmap based on that resultOutline:Kamath Paper:troduction NotesSafety, Interpretability, Biometric and Scientific Domain (MedQA 2019?) Review Piror WOrkAnwer RankingUsing NLIContradictions? Dialogue - consistanceUsing critical statements? Contradictions? To select answersBackground and MotivationsFactuality? GopherCite, WebGPTChen Verify QASelectiveQATo mentionWhat Would it Take to get Biomedical QA Systems into Practice?Try NLI zero shot modelTextual entailment?Proposes a new paradigm in answer choice selection where the answer choice selection mechanism relies on the ability to generate contradictions about a given input.Main argument: Focusing on the verification paradigm loose sight of the power of critical tests. I show that incorporating these critical tests improves these system in QA domain, it should help in others.In summary our contribution is as follows:Critical Rationalism and Karl Poppers PhilosophyRelated WorkTextual Entailment for QAHarabagu and hicks - AbchaChenMisraKamal?Contrastive and Counterfactual StatementsFactual and Persona ConsistencyUsing NLI for RankingSelective QACharacterizing NLITaxiDid a lot better than this prior work: https://aclanthology.org/D13-1020.pdfTaxi exploiting the dynamics of contradictionsPrior Art: Consistency detection:https://scite.ai/reports/will-i-sound-like-me-W84L888d?page=1Maybe I should also add discussion in here{\ldots}Also I should not the findings are similar to the universal nli few shot paper of MNLI observed to do well as a baseline task on domain transferFor the results presented below, readers are directed to Appendix E to review a correlation and regression analysis which 
\end{document}
