
@inproceedings{schuster_get_2021,
	address = {Online},
	title = {Get {Your} {Vitamin} {C}! {Robust} {Fact} {Verification} with {Contrastive} {Evidence}},
	url = {https://aclanthology.org/2021.naacl-main.52},
	doi = {10.18653/v1/2021.naacl-main.52},
	abstract = {Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness—improving accuracy by 10\% on adversarial fact verification and 6\% on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.},
	urldate = {2022-05-15},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Schuster, Tal and Fisch, Adam and Barzilay, Regina},
	month = jun,
	year = {2021},
	pages = {624--643},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/LPCGWFC7/Schuster et al. - 2021 - Get Your Vitamin C! Robust Fact Verification with .pdf:application/pdf},
}

@techreport{demszky_transforming_2018,
	title = {Transforming {Question} {Answering} {Datasets} {Into} {Natural} {Language} {Inference} {Datasets}},
	url = {http://arxiv.org/abs/1809.02922},
	abstract = {Existing datasets for natural language inference (NLI) have propelled research on language understanding. We propose a new method for automatically deriving NLI datasets from the growing abundance of large-scale question answering datasets. Our approach hinges on learning a sentence transformation model which converts question-answer pairs into their declarative forms. Despite being primarily trained on a single QA dataset, we show that it can be successfully applied to a variety of other QA resources. Using this system, we automatically derive a new freely available dataset of over 500k NLI examples (QA-NLI), and show that it exhibits a wide range of inference phenomena rarely seen in previous NLI datasets.},
	number = {arXiv:1809.02922},
	urldate = {2022-05-16},
	institution = {arXiv},
	author = {Demszky, Dorottya and Guu, Kelvin and Liang, Percy},
	month = sep,
	year = {2018},
	doi = {10.48550/arXiv.1809.02922},
	note = {arXiv:1809.02922 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/M6LFTD6L/Demszky et al. - 2018 - Transforming Question Answering Datasets Into Natu.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/JSNAKVK5/1809.html:text/html},
}

@inproceedings{khot_scitail_2018,
	title = {{SciTaiL}: {A} {Textual} {Entailment} {Dataset} from {Science} {Question} {Answering}},
	shorttitle = {{SciTaiL}},
	abstract = {A new dataset and model for textual entailment, derived from treating multiple-choice question-answering as an entailment problem, is presented, and it is demonstrated that one can improve accuracy on SCITAIL by 5\% using a new neural model that exploits linguistic structure. We present a new dataset and model for textual entailment, derived from treating multiple-choice question-answering as an entailment problem. SCITAIL is the first entailment set that is created solely from natural sentences that already exist independently “in the wild” rather than sentences authored specifically for the entailment task. Different from existing entailment datasets, we create hypotheses from science questions and the corresponding answer candidates, and premises from relevant web sentences retrieved from a large corpus. These sentences are often linguistically challenging. This, combined with the high lexical similarity of premise and hypothesis for both entailed and non-entailed pairs, makes this new entailment task particularly difficult. The resulting challenge is evidenced by state-of-the-art textual entailment systems achieving mediocre performance on SCITAIL, especially in comparison to a simple majority class baseline. As a step forward, we demonstrate that one can improve accuracy on SCITAIL by 5\% using a new neural model that exploits linguistic structure.},
	booktitle = {{AAAI}},
	author = {Khot, Tushar and Sabharwal, Ashish and Clark, Peter},
	year = {2018},
}

@article{paramasivam_survey_2021,
	title = {A survey on textual entailment based question answering},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157821003311},
	doi = {10.1016/j.jksuci.2021.11.017},
	abstract = {Question answering, an information retrieval system that seeks knowledge, is one of the classic applications in Natural Language Processing. A question answering system comprises numerous sets of subtasks. Some of the subtasks are Passage Retrieval, Answer Ranking, Question Similarity, Question Generation, Question Classification, Answer Selection, and Answer Validation. Numerous approaches have been experimented on in the question answering system to achieve accurate results. One such approach for the question answering system is Textual Entailment. Textual Entailment is a framework that captures significant semantic inference. Textual Entailment of two text fragments can be defined as the task of deciding whether the meaning of one text fragment can be inferred from another text fragment. This survey discusses how and why Textual Entailment is applied to various subtasks in question answering.},
	language = {en},
	urldate = {2022-06-11},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Paramasivam, Aarthi and Nirmala, S. Jaya},
	month = dec,
	year = {2021},
	keywords = {Natural Language Processing, Question Answering, Textual Entailment},
	file = {Full Text:/Users/domenicrosati/Zotero/storage/SXI9Q9WH/Paramasivam and Nirmala - 2021 - A survey on textual entailment based question answ.pdf:application/pdf;ScienceDirect Snapshot:/Users/domenicrosati/Zotero/storage/J2LQKABE/S1319157821003311.html:text/html},
}

@inproceedings{mishra_looking_2021,
	address = {Online},
	title = {Looking {Beyond} {Sentence}-{Level} {Natural} {Language} {Inference} for {Question} {Answering} and {Text} {Summarization}},
	url = {https://aclanthology.org/2021.naacl-main.104},
	doi = {10.18653/v1/2021.naacl-main.104},
	abstract = {Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts); (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets; and (3) models trained on the converted, longer-premise datasets outperform those trained using short-premise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths.},
	urldate = {2022-06-11},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Mishra, Anshuman and Patel, Dhruvesh and Vijayakumar, Aparna and Li, Xiang Lorraine and Kapanipathi, Pavan and Talamadupula, Kartik},
	month = jun,
	year = {2021},
	pages = {1322--1336},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/ZB53A2NY/Mishra et al. - 2021 - Looking Beyond Sentence-Level Natural Language Inf.pdf:application/pdf},
}

@techreport{wright_generating_2022,
	title = {Generating {Scientific} {Claims} for {Zero}-{Shot} {Scientific} {Fact} {Checking}},
	url = {http://arxiv.org/abs/2203.12990},
	abstract = {Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data, as annotation requires domain expertise. To address this challenge, we propose scientific claim generation, the task of generating one or more atomic and verifiable claims from scientific sentences, and demonstrate its usefulness in zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new supervised method for generating claims supported by the literature, as well as KBIN, a novel method for generating claim negations. Additionally, we adapt an existing unsupervised entity-centric method of claim generation to biomedical claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN, achieve up to 90\% performance of fully supervised models trained on manually annotated claims and evidence. A rigorous evaluation study demonstrates significant improvement in generated claim and negation quality over existing baselines},
	number = {arXiv:2203.12990},
	urldate = {2022-06-11},
	institution = {arXiv},
	author = {Wright, Dustin and Wadden, David and Lo, Kyle and Kuehl, Bailey and Cohan, Arman and Augenstein, Isabelle and Wang, Lucy Lu},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.12990},
	note = {arXiv:2203.12990 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/TIZS5I26/Wright et al. - 2022 - Generating Scientific Claims for Zero-Shot Scienti.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/Z63JFWDK/2203.html:text/html},
}

@inproceedings{pan_zero-shot_2021,
	address = {Online},
	title = {Zero-shot {Fact} {Verification} by {Claim} {Generation}},
	url = {https://aclanthology.org/2021.acl-short.61},
	doi = {10.18653/v1/2021.acl-short.61},
	abstract = {Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model's F1 from 50\% to 77\%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available.},
	urldate = {2022-06-11},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Pan, Liangming and Chen, Wenhu and Xiong, Wenhan and Kan, Min-Yen and Wang, William Yang},
	month = aug,
	year = {2021},
	pages = {476--483},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/SLII2XB5/Pan et al. - 2021 - Zero-shot Fact Verification by Claim Generation.pdf:application/pdf},
}

@inproceedings{ben_abacha_overview_2019,
	address = {Florence, Italy},
	title = {Overview of the {MEDIQA} 2019 {Shared} {Task} on {Textual} {Inference}, {Question} {Entailment} and {Question} {Answering}},
	url = {https://aclanthology.org/W19-5039},
	doi = {10.18653/v1/W19-5039},
	abstract = {This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98\% in the NLI task, 74.9\% in the RQE task, and 78.3\% in the QA task. In this paper, we describe the tasks, the datasets, and the participants' approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain.},
	urldate = {2022-06-11},
	booktitle = {Proceedings of the 18th {BioNLP} {Workshop} and {Shared} {Task}},
	publisher = {Association for Computational Linguistics},
	author = {Ben Abacha, Asma and Shivade, Chaitanya and Demner-Fushman, Dina},
	month = aug,
	year = {2019},
	pages = {370--379},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/UD5CD6E4/Ben Abacha et al. - 2019 - Overview of the MEDIQA 2019 Shared Task on Textual.pdf:application/pdf},
}

@article{abacha_question-entailment_2019,
	title = {A {Question}-{Entailment} {Approach} to {Question} {Answering}},
	volume = {20},
	issn = {1471-2105},
	url = {http://arxiv.org/abs/1901.08079},
	doi = {10.1186/s12859-019-3119-4},
	abstract = {One of the challenges in large-scale information retrieval (IR) is to develop fine-grained and domain-specific methods to answer natural language questions. Despite the availability of numerous sources and datasets for answer retrieval, Question Answering (QA) remains a challenging problem due to the difficulty of the question understanding and answer extraction tasks. One of the promising tracks investigated in QA is to map new questions to formerly answered questions that are `similar'. In this paper, we propose a novel QA approach based on Recognizing Question Entailment (RQE) and we describe the QA system and resources that we built and evaluated on real medical questions. First, we compare machine learning and deep learning methods for RQE using different kinds of datasets, including textual inference, question similarity and entailment in both the open and clinical domains. Second, we combine IR models with the best RQE method to select entailed questions and rank the retrieved answers. To study the end-to-end QA approach, we built the MedQuAD collection of 47,457 question-answer pairs from trusted medical sources, that we introduce and share in the scope of this paper. Following the evaluation process used in TREC 2017 LiveQA, we find that our approach exceeds the best results of the medical task with a 29.8\% increase over the best official score. The evaluation results also support the relevance of question entailment for QA and highlight the effectiveness of combining IR and RQE for future QA efforts. Our findings also show that relying on a restricted set of reliable answer sources can bring a substantial improvement in medical QA.},
	number = {1},
	urldate = {2022-06-11},
	journal = {BMC Bioinformatics},
	author = {Abacha, Asma Ben and Demner-Fushman, Dina},
	month = dec,
	year = {2019},
	note = {arXiv:1901.08079 [cs]},
	keywords = {68T50, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	pages = {511},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/98F6IMS2/Abacha and Demner-Fushman - 2019 - A Question-Entailment Approach to Question Answeri.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/PBU5G5LI/1901.html:text/html},
}

@techreport{hsu_wikicontradiction_2021,
	title = {{WikiContradiction}: {Detecting} {Self}-{Contradiction} {Articles} on {Wikipedia}},
	shorttitle = {{WikiContradiction}},
	url = {http://arxiv.org/abs/2111.08543},
	abstract = {While Wikipedia has been utilized for fact-checking and claim verification to debunk misinformation and disinformation, it is essential to either improve article quality and rule out noisy articles. Self-contradiction is one of the low-quality article types in Wikipedia. In this work, we propose a task of detecting self-contradiction articles in Wikipedia. Based on the "self-contradictory" template, we create a novel dataset for the self-contradiction detection task. Conventional contradiction detection focuses on comparing pairs of sentences or claims, but self-contradiction detection needs to further reason the semantics of an article and simultaneously learn the contradiction-aware comparison from all pairs of sentences. Therefore, we present the first model, Pairwise Contradiction Neural Network (PCNN), to not only effectively identify self-contradiction articles, but also highlight the most contradiction pairs of contradiction sentences. The main idea of PCNN is two-fold. First, to mitigate the effect of data scarcity on self-contradiction articles, we pre-train the module of pairwise contradiction learning using SNLI and MNLI benchmarks. Second, we select top-K sentence pairs with the highest contradiction probability values and model their correlation to determine whether the corresponding article belongs to self-contradiction. Experiments conducted on the proposed WikiContradiction dataset exhibit that PCNN can generate promising performance and comprehensively highlight the sentence pairs the contradiction locates.},
	number = {arXiv:2111.08543},
	urldate = {2022-06-11},
	institution = {arXiv},
	author = {Hsu, Cheng and Li, Cheng-Te and Saez-Trumper, Diego and Hsu, Yi-Zhan},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2111.08543},
	note = {arXiv:2111.08543 [cs]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/7ILG8BJY/Hsu et al. - 2021 - WikiContradiction Detecting Self-Contradiction Ar.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/ATXL5LEP/2111.html:text/html},
}

@techreport{pan_contraqa_2021,
	title = {{ContraQA}: {Question} {Answering} under {Contradicting} {Contexts}},
	shorttitle = {{ContraQA}},
	url = {http://arxiv.org/abs/2110.07803},
	abstract = {With a rise in false, inaccurate, and misleading information in propaganda, news, and social media, real-world Question Answering (QA) systems face the challenges of synthesizing and reasoning over contradicting information to derive correct answers. This urgency gives rise to the need to make QA systems robust to misinformation, a topic previously unexplored. We study the risk of misinformation to QA models by investigating the behavior of the QA model under contradicting contexts that are mixed with both real and fake information. We create the first large-scale dataset for this problem, namely Contra-QA, which contains over 10K human-written and model-generated contradicting pairs of contexts. Experiments show that QA models are vulnerable under contradicting contexts brought by misinformation. To defend against such a threat, we build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion.},
	number = {arXiv:2110.07803},
	urldate = {2022-06-11},
	institution = {arXiv},
	author = {Pan, Liangming and Chen, Wenhu and Kan, Min-Yen and Wang, William Yang},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2110.07803},
	note = {arXiv:2110.07803 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/H5RQUZSZ/Pan et al. - 2021 - ContraQA Question Answering under Contradicting C.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/ZKQ95N4I/2110.html:text/html},
}

@article{song_generating_2020,
	title = {Generating {Persona} {Consistent} {Dialogues} by {Exploiting} {Natural} {Language} {Inference}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6417},
	doi = {10.1609/aaai.v34i05.6417},
	abstract = {Consistency is one of the major challenges faced by dialogue agents. A human-like dialogue agent should not only respond naturally, but also maintain a consistent persona. In this paper, we exploit the advantages of natural language inference (NLI) technique to address the issue of generating persona consistent dialogues. Different from existing work that re-ranks the retrieved responses through an NLI model, we cast the task as a reinforcement learning problem and propose to exploit the NLI signals from response-persona pairs as rewards for the process of dialogue generation. Specifically, our generator employs an attention-based encoder-decoder to generate persona-based responses. Our evaluator consists of two components: an adversarially trained naturalness module and an NLI based consistency module. Moreover, we use another well-performed NLI model in the evaluation of persona-consistency. Experimental results on both human and automatic metrics, including the model-based consistency evaluation, demonstrate that the proposed approach outperforms strong generative baselines, especially in the persona-consistency of generated responses.},
	language = {en},
	number = {05},
	urldate = {2022-06-11},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Song, Haoyu and Zhang, Wei-Nan and Hu, Jingwen and Liu, Ting},
	month = apr,
	year = {2020},
	note = {Number: 05},
	pages = {8878--8885},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/UZ4ER4Y9/Song et al. - 2020 - Generating Persona Consistent Dialogues by Exploit.pdf:application/pdf},
}

@inproceedings{yin_universal_2020,
	address = {Online},
	title = {Universal {Natural} {Language} {Processing} with {Limited} {Annotations}: {Try} {Few}-shot {Textual} {Entailment} as a {Start}},
	shorttitle = {Universal {Natural} {Language} {Processing} with {Limited} {Annotations}},
	url = {https://aclanthology.org/2020.emnlp-main.660},
	doi = {10.18653/v1/2020.emnlp-main.660},
	abstract = {A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations. Universal NLP can be probably achieved through different routines. In this work, we introduce Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on new entailment domains in a few-shot setting, and show its effectiveness as a unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited.},
	urldate = {2022-06-11},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Yin, Wenpeng and Rajani, Nazneen Fatema and Radev, Dragomir and Socher, Richard and Xiong, Caiming},
	month = nov,
	year = {2020},
	pages = {8229--8239},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/DGGQ4JJV/Yin et al. - 2020 - Universal Natural Language Processing with Limited.pdf:application/pdf},
}

@inproceedings{saakyan_covid-fact_2021,
	address = {Online},
	title = {{COVID}-{Fact}: {Fact} {Extraction} and {Verification} of {Real}-{World} {Claims} on {COVID}-19 {Pandemic}},
	shorttitle = {{COVID}-{Fact}},
	url = {https://aclanthology.org/2021.acl-long.165},
	doi = {10.18653/v1/2021.acl-long.165},
	abstract = {We introduce a FEVER-like dataset COVID-Fact of 4,086 claims concerning the COVID-19 pandemic. The dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence. Unlike previous approaches, we automatically detect true claims and their source articles and then generate counter-claims using automatic methods rather than employing human annotators. Along with our constructed resource, we formally present the task of identifying relevant evidence for the claims and verifying whether the evidence refutes or supports a given claim. In addition to scientific claims, our data contains simplified general claims from media sources, making it better suited for detecting general misinformation regarding COVID-19. Our experiments indicate that COVID-Fact will provide a challenging testbed for the development of new systems and our approach will reduce the costs of building domain-specific datasets for detecting misinformation.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Saakyan, Arkadiy and Chakrabarty, Tuhin and Muresan, Smaranda},
	month = aug,
	year = {2021},
	pages = {2116--2129},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/E3IGQHWV/Saakyan et al. - 2021 - COVID-Fact Fact Extraction and Verification of Re.pdf:application/pdf},
}

@inproceedings{kell_what_2021,
	address = {Punta Cana, Dominican Republic},
	title = {What {Would} it {Take} to get {Biomedical} {QA} {Systems} into {Practice}?},
	url = {https://aclanthology.org/2021.mrqa-1.3},
	doi = {10.18653/v1/2021.mrqa-1.3},
	abstract = {Medical question answering (QA) systems have the potential to answer clinicians' uncertainties about treatment and diagnosis on-demand, informed by the latest evidence. However, despite the significant progress in general QA made by the NLP community, medical QA systems are still not widely used in clinical environments. One likely reason for this is that clinicians may not readily trust QA system outputs, in part because transparency, trustworthiness, and provenance have not been key considerations in the design of such models. In this paper we discuss a set of criteria that, if met, we argue would likely increase the utility of biomedical QA systems, which may in turn lead to adoption of such systems in practice. We assess existing models, tasks, and datasets with respect to these criteria, highlighting shortcomings of previously proposed approaches and pointing toward what might be more usable QA systems.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 3rd {Workshop} on {Machine} {Reading} for {Question} {Answering}},
	publisher = {Association for Computational Linguistics},
	author = {Kell, Gregory and Marshall, Iain and Wallace, Byron and Jaun, Andre},
	month = nov,
	year = {2021},
	pages = {28--41},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/JS2SU7Q7/Kell et al. - 2021 - What Would it Take to get Biomedical QA Systems in.pdf:application/pdf},
}

@article{laban_summac_2022,
	title = {{SummaC}: {Re}-{Visiting} {NLI}-based {Models} for {Inconsistency} {Detection} in {Summarization}},
	volume = {10},
	issn = {2307-387X},
	shorttitle = {{SummaC}},
	url = {https://doi.org/10.1162/tacl_a_00453},
	doi = {10.1162/tacl_a_00453},
	abstract = {In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4\%, a 5\% improvement compared with prior work.},
	urldate = {2022-06-18},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Laban, Philippe and Schnabel, Tobias and Bennett, Paul N. and Hearst, Marti A.},
	month = feb,
	year = {2022},
	pages = {163--177},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/NERKZEQM/Laban et al. - 2022 - SummaC Re-Visiting NLI-based Models for Inconsist.pdf:application/pdf;Snapshot:/Users/domenicrosati/Zotero/storage/IIGHEY6F/SummaC-Re-Visiting-NLI-based-Models-for.html:text/html},
}

@misc{menick_teaching_2022,
	title = {Teaching language models to support answers with verified quotes},
	url = {http://arxiv.org/abs/2203.11147},
	doi = {10.48550/arXiv.2203.11147},
	abstract = {Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train "open-book" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80{\textbackslash}\% of the time on this Natural Questions subset, and 67{\textbackslash}\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90{\textbackslash}\% and 80{\textbackslash}\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and McAleese, Nat},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2203.11147
arXiv:2203.11147 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/XLJ2C3LA/Menick et al. - 2022 - Teaching language models to support answers with v.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/QWPIJRU5/2203.html:text/html},
}

@misc{nakano_webgpt_2022,
	title = {{WebGPT}: {Browser}-assisted question-answering with human feedback},
	shorttitle = {{WebGPT}},
	url = {http://arxiv.org/abs/2112.09332},
	doi = {10.48550/arXiv.2112.09332},
	abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2112.09332
arXiv:2112.09332 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/MVV5HXKL/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/6IRA4DWT/2112.html:text/html},
}

@misc{saunders_self-critiquing_2022,
	title = {Self-critiquing models for assisting human evaluators},
	url = {http://arxiv.org/abs/2206.05802},
	doi = {10.48550/arXiv.2206.05802},
	abstract = {We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.05802
arXiv:2206.05802 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/VJYTLADL/Saunders et al. - 2022 - Self-critiquing models for assisting human evaluat.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/3R2ZM9Z2/2206.html:text/html},
}

@book{miller_critical_2015,
	title = {Critical {Rationalism}: {A} {Restatement} and {Defence}},
	isbn = {978-0-8126-9940-1},
	shorttitle = {Critical {Rationalism}},
	abstract = {David Miller elegantly and provocatively reformulates critical rationalism—the revolutionary approach to epistemology advocated by Karl Popper—by answering its most important critics. He argues for an approach to rationality freed from the debilitating authoritarian dependence on reasons and justification."Miller presents a particularly useful and stimulating account of critical rationalism. His work is both interesting and controversial . . . of interest to anyone with concerns in epistemology or the philosophy of science."—Canadian Philosophical Reviews},
	language = {en},
	publisher = {Open Court},
	author = {Miller, David},
	month = dec,
	year = {2015},
	note = {Google-Books-ID: bh\_yCgAAQBAJ},
	keywords = {Science / Philosophy \& Social Aspects},
}

@inproceedings{chen_can_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Can {NLI} {Models} {Verify} {QA} {Systems}' {Predictions}?},
	url = {https://aclanthology.org/2021.findings-emnlp.324},
	doi = {10.18653/v1/2021.findings-emnlp.324},
	abstract = {To build robust question answering systems, we need the ability to verify whether answers to questions are truly correct, not just “good enough” in the context of imperfect QA datasets. We explore the use of natural language inference (NLI) as a way to achieve this goal, as NLI inherently requires the premise (document context) to contain all necessary information to support the hypothesis (proposed answer to the question). We leverage large pre-trained models and recent prior datasets to construct powerful question conversion and decontextualization modules, which can reformulate QA instances as premise-hypothesis pairs with very high reliability. Then, by combining standard NLI datasets with NLI examples automatically derived from QA training data, we can train NLI models to evaluate QA models' proposed answers. We show that our approach improves the confidence estimation of a QA model across different domains, evaluated in a selective QA setting. Careful manual analysis over the predictions of our NLI model shows that it can further identify cases where the QA model produces the right answer for the wrong reason, i.e., when the answer sentence cannot address all aspects of the question.},
	urldate = {2022-06-18},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Jifan and Choi, Eunsol and Durrett, Greg},
	month = nov,
	year = {2021},
	pages = {3841--3854},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/TWPY2AL4/Chen et al. - 2021 - Can NLI Models Verify QA Systems' Predictions.pdf:application/pdf},
}

@inproceedings{trivedi_repurposing_2019,
	address = {Minneapolis, Minnesota},
	title = {Repurposing {Entailment} for {Multi}-{Hop} {Question} {Answering} {Tasks}},
	url = {https://aclanthology.org/N19-1302},
	doi = {10.18653/v1/N19-1302},
	abstract = {Question Answering (QA) naturally reduces to an entailment problem, namely, verifying whether some text entails the answer to a question. However, for multi-hop QA tasks, which require reasoning with multiple sentences, it remains unclear how best to utilize entailment models pre-trained on large scale datasets such as SNLI, which are based on sentence pairs. We introduce Multee, a general architecture that can effectively use entailment models for multi-hop QA tasks. Multee uses (i) a local module that helps locate important sentences, thereby avoiding distracting information, and (ii) a global module that aggregates information by effectively incorporating importance weights. Importantly, we show that both modules can use entailment functions pre-trained on a large scale NLI datasets. We evaluate performance on MultiRC and OpenBookQA, two multihop QA datasets. When using an entailment function pre-trained on NLI datasets, Multee outperforms QA models trained only on the target QA datasets and the OpenAI transformer models.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Trivedi, Harsh and Kwon, Heeyoung and Khot, Tushar and Sabharwal, Ashish and Balasubramanian, Niranjan},
	month = jun,
	year = {2019},
	pages = {2948--2958},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/66CS8H5V/Trivedi et al. - 2019 - Repurposing Entailment for Multi-Hop Question Answ.pdf:application/pdf},
}

@inproceedings{kamath_selective_2020,
	address = {Online},
	title = {Selective {Question} {Answering} under {Domain} {Shift}},
	url = {https://aclanthology.org/2020.acl-main.503},
	doi = {10.18653/v1/2020.acl-main.503},
	abstract = {To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model's training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model's softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the model's behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56\% of questions while maintaining 80\% accuracy; in contrast, directly using the model's probabilities only answers 48\% at 80\% accuracy.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kamath, Amita and Jia, Robin and Liang, Percy},
	month = jul,
	year = {2020},
	pages = {5684--5696},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/NP2K522G/Kamath et al. - 2020 - Selective Question Answering under Domain Shift.pdf:application/pdf},
}

@misc{kaushik_learning_2020,
	title = {Learning the {Difference} that {Makes} a {Difference} with {Counterfactually}-{Augmented} {Data}},
	url = {http://arxiv.org/abs/1909.12434},
	doi = {10.48550/arXiv.1909.12434},
	abstract = {Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C.},
	month = feb,
	year = {2020},
	note = {Number: arXiv:1909.12434
arXiv:1909.12434 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/J3UQVTHC/Kaushik et al. - 2020 - Learning the Difference that Makes a Difference wi.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/AEBWEFZK/1909.html:text/html},
}

@inproceedings{cao_cliff_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{CLIFF}: {Contrastive} {Learning} for {Improving} {Faithfulness} and {Factuality} in {Abstractive} {Summarization}},
	shorttitle = {{CLIFF}},
	url = {https://aclanthology.org/2021.emnlp-main.532},
	doi = {10.18653/v1/2021.emnlp-main.532},
	abstract = {We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is presented, which leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train summarization systems that are better at distinguishing between them. We further design four types of strategies for creating negative samples, to resemble errors made commonly by two state-of-the-art models, BART and PEGASUS, found in our new human annotations of summary errors. Experiments on XSum and CNN/Daily Mail show that our contrastive learning framework is robust across datasets and models. It consistently produces more factual summaries than strong comparisons with post error correction, entailment-based reranking, and unlikelihood training, according to QA-based factuality evaluation. Human judges echo the observation and find that our model summaries correct more errors.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Shuyang and Wang, Lu},
	month = nov,
	year = {2021},
	pages = {6633--6649},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/K9MNJGDA/Cao and Wang - 2021 - CLIFF Contrastive Learning for Improving Faithful.pdf:application/pdf},
}

@misc{fabbri_qafacteval_2022,
	title = {{QAFactEval}: {Improved} {QA}-{Based} {Factual} {Consistency} {Evaluation} for {Summarization}},
	shorttitle = {{QAFactEval}},
	url = {http://arxiv.org/abs/2112.08542},
	doi = {10.48550/arXiv.2112.08542},
	abstract = {Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14\% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost.},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {Fabbri, Alexander R. and Wu, Chien-Sheng and Liu, Wenhao and Xiong, Caiming},
	month = apr,
	year = {2022},
	note = {Number: arXiv:2112.08542
arXiv:2112.08542 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/4DJ6NL7W/Fabbri et al. - 2022 - QAFactEval Improved QA-Based Factual Consistency .pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/39X7NKG2/2112.html:text/html},
}

@misc{li_faithfulness_2022,
	title = {Faithfulness in {Natural} {Language} {Generation}: {A} {Systematic} {Survey} of {Analysis}, {Evaluation} and {Optimization} {Methods}},
	shorttitle = {Faithfulness in {Natural} {Language} {Generation}},
	url = {http://arxiv.org/abs/2203.05227},
	doi = {10.48550/arXiv.2203.05227},
	abstract = {Natural Language Generation (NLG) has made great progress in recent years due to the development of deep learning techniques such as pre-trained language models. This advancement has resulted in more fluent, coherent and even properties controllable (e.g. stylistic, sentiment, length etc.) generation, naturally leading to development in downstream tasks such as abstractive summarization, dialogue generation, machine translation, and data-to-text generation. However, the faithfulness problem that the generated text usually contains unfaithful or non-factual information has become the biggest challenge, which makes the performance of text generation unsatisfactory for practical applications in many real-world scenarios. Many studies on analysis, evaluation, and optimization methods for faithfulness problems have been proposed for various tasks, but have not been organized, compared and discussed in a combined manner. In this survey, we provide a systematic overview of the research progress on the faithfulness problem of NLG, including problem analysis, evaluation metrics and optimization methods. We organize the evaluation and optimization methods for different tasks into a unified taxonomy to facilitate comparison and learning across tasks. Several research trends are discussed further.},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {Li, Wei and Wu, Wenhao and Chen, Moye and Liu, Jiachen and Xiao, Xinyan and Wu, Hua},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2203.05227
arXiv:2203.05227 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/C8WFXAEZ/Li et al. - 2022 - Faithfulness in Natural Language Generation A Sys.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/5ADDZLAB/2203.html:text/html},
}

@inproceedings{nie_i_2021,
	address = {Online},
	title = {I like fish, especially dolphins: {Addressing} {Contradictions} in {Dialogue} {Modeling}},
	shorttitle = {I like fish, especially dolphins},
	url = {https://aclanthology.org/2021.acl-long.134},
	doi = {10.18653/v1/2021.acl-long.134},
	abstract = {To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We show that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and out-of-distribution dialogues than standard (unstructured) Transformers. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Nie, Yixin and Williamson, Mary and Bansal, Mohit and Kiela, Douwe and Weston, Jason},
	month = aug,
	year = {2021},
	pages = {1699--1713},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/M99Q7HS8/Nie et al. - 2021 - I like fish, especially dolphins Addressing Contr.pdf:application/pdf},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {Number: arXiv:1907.11692
arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/K2XD3N8P/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/64Z73I89/1907.html:text/html},
}

@misc{he_debertav3_2021,
	title = {{DeBERTaV3}: {Improving} {DeBERTa} using {ELECTRA}-{Style} {Pre}-{Training} with {Gradient}-{Disentangled} {Embedding} {Sharing}},
	shorttitle = {{DeBERTaV3}},
	url = {http://arxiv.org/abs/2111.09543},
	doi = {10.48550/arXiv.2111.09543},
	abstract = {This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37\% average score, which is 1.37\% over DeBERTa and 1.91\% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mDeBERTa and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTa Base achieves a 79.8\% zero-shot cross-lingual accuracy on XNLI and a 3.6\% improvement over XLM-R Base, creating a new SOTA on this benchmark. We have made our pre-trained models and inference code publicly available at https://github.com/microsoft/DeBERTa.},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
	month = dec,
	year = {2021},
	note = {Number: arXiv:2111.09543
arXiv:2111.09543 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, cs.CL, cs.GL, I.2, I.7},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/SVZRJM5P/He et al. - 2021 - DeBERTaV3 Improving DeBERTa using ELECTRA-Style P.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/H8W42PSF/2111.html:text/html},
}

@misc{he_deberta_2021,
	title = {{DeBERTa}: {Decoding}-enhanced {BERT} with {Disentangled} {Attention}},
	shorttitle = {{DeBERTa}},
	url = {http://arxiv.org/abs/2006.03654},
	doi = {10.48550/arXiv.2006.03654},
	abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).},
	urldate = {2022-06-18},
	publisher = {arXiv},
	author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {Number: arXiv:2006.03654
arXiv:2006.03654 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, cs.CL, cs.GL, I.2, I.7},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/E7CBHU3Y/He et al. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled .pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/7GDUHWTH/2006.html:text/html},
}

@inproceedings{rajpurkar_squad_2016,
	address = {Austin, Texas},
	title = {{SQuAD}: 100,000+ {Questions} for {Machine} {Comprehension} of {Text}},
	shorttitle = {{SQuAD}},
	url = {https://aclanthology.org/D16-1264},
	doi = {10.18653/v1/D16-1264},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	month = nov,
	year = {2016},
	pages = {2383--2392},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/UBFB6N42/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:application/pdf},
}

@inproceedings{rajpurkar_know_2018,
	address = {Melbourne, Australia},
	title = {Know {What} {You} {Don}'t {Know}: {Unanswerable} {Questions} for {SQuAD}},
	shorttitle = {Know {What} {You} {Don}'t {Know}},
	url = {https://aclanthology.org/P18-2124},
	doi = {10.18653/v1/P18-2124},
	abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD achieves only 66\% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	month = jul,
	year = {2018},
	pages = {784--789},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/Y4RKLJRG/Rajpurkar et al. - 2018 - Know What You Don't Know Unanswerable Questions f.pdf:application/pdf},
}

@inproceedings{fisch_mrqa_2019,
	address = {Hong Kong, China},
	title = {{MRQA} 2019 {Shared} {Task}: {Evaluating} {Generalization} in {Reading} {Comprehension}},
	shorttitle = {{MRQA} 2019 {Shared} {Task}},
	url = {https://aclanthology.org/D19-5801},
	doi = {10.18653/v1/D19-5801},
	abstract = {We present the results of the Machine Reading for Question Answering (MRQA) 2019 shared task on evaluating the generalization capabilities of reading comprehension systems. In this task, we adapted and unified 18 distinct question answering datasets into the same format. Among them, six datasets were made available for training, six datasets were made available for development, and the rest were hidden for final evaluation. Ten teams submitted systems, which explored various ideas including data sampling, multi-task learning, adversarial training and ensembling. The best system achieved an average F1 score of 72.5 on the 12 held-out datasets, 10.7 absolute points higher than our initial baseline based on BERT.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 2nd {Workshop} on {Machine} {Reading} for {Question} {Answering}},
	publisher = {Association for Computational Linguistics},
	author = {Fisch, Adam and Talmor, Alon and Jia, Robin and Seo, Minjoon and Choi, Eunsol and Chen, Danqi},
	month = nov,
	year = {2019},
	pages = {1--13},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/UJD3WN98/Fisch et al. - 2019 - MRQA 2019 Shared Task Evaluating Generalization i.pdf:application/pdf},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-074.html},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {140},
	urldate = {2022-06-18},
	journal = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
	pages = {1--67},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/94DD8743/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;Source Code:/Users/domenicrosati/Zotero/storage/LHHJQWUK/text-to-text-transfer-transformer.html:text/html},
}

@inproceedings{nie_adversarial_2020,
	address = {Online},
	title = {Adversarial {NLI}: {A} {New} {Benchmark} for {Natural} {Language} {Understanding}},
	shorttitle = {Adversarial {NLI}},
	url = {https://aclanthology.org/2020.acl-main.441},
	doi = {10.18653/v1/2020.acl-main.441},
	abstract = {We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
	month = jul,
	year = {2020},
	pages = {4885--4901},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/75LCAHMT/Nie et al. - 2020 - Adversarial NLI A New Benchmark for Natural Langu.pdf:application/pdf},
}

@inproceedings{jia_adversarial_2017,
	address = {Copenhagen, Denmark},
	title = {Adversarial {Examples} for {Evaluating} {Reading} {Comprehension} {Systems}},
	url = {https://aclanthology.org/D17-1215},
	doi = {10.18653/v1/D17-1215},
	abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75\% F1 score to 36\%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7\%. We hope our insights will motivate the development of new models that understand language more precisely.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Jia, Robin and Liang, Percy},
	month = sep,
	year = {2017},
	pages = {2021--2031},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/B5JKN7FP/Jia and Liang - 2017 - Adversarial Examples for Evaluating Reading Compre.pdf:application/pdf},
}

@inproceedings{huang_cosmos_2019,
	address = {Hong Kong, China},
	title = {Cosmos {QA}: {Machine} {Reading} {Comprehension} with {Contextual} {Commonsense} {Reasoning}},
	shorttitle = {Cosmos {QA}},
	url = {https://aclanthology.org/D19-1243},
	doi = {10.18653/v1/D19-1243},
	abstract = {Understanding narratives requires reading between the lines, which in turn, requires interpreting the likely causes and effects of events, even when they are not mentioned explicitly. In this paper, we introduce Cosmos QA, a large-scale dataset of 35,600 problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. In stark contrast to most existing reading comprehension datasets where the questions focus on factual and literal understanding of the context paragraph, our dataset focuses on reading between the lines over a diverse collection of people's everyday narratives, asking such questions as “what might be the possible reason of ...?”, or “what would have happened if ...” that require reasoning beyond the exact text spans in the context. To establish baseline performances on Cosmos QA, we experiment with several state-of-the-art neural architectures for reading comprehension, and also propose a new architecture that improves over the competitive baselines. Experimental results demonstrate a significant gap between machine (68.4\%) and human performance (94\%), pointing to avenues for future research on commonsense machine comprehension. Dataset, code and leaderboard is publicly available at https://wilburone.github.io/cosmos.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Lifu and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
	month = nov,
	year = {2019},
	pages = {2391--2401},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/NK4M2VCV/Huang et al. - 2019 - Cosmos QA Machine Reading Comprehension with Cont.pdf:application/pdf},
}

@article{sun_dream_2019,
	title = {{DREAM}: {A} {Challenge} {Data} {Set} and {Models} for {Dialogue}-{Based} {Reading} {Comprehension}},
	volume = {7},
	shorttitle = {{DREAM}},
	url = {https://aclanthology.org/Q19-1014},
	doi = {10.1162/tacl_a_00264},
	abstract = {We present DREAM, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our data set contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension data sets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems: 84\% of answers are non-extractive, 85\% of questions require reasoning beyond a single sentence, and 34\% of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM data set show the effectiveness of dialogue structure and general world knowledge. DREAM is available at https://dataset.org/dream/.},
	urldate = {2022-06-18},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Sun, Kai and Yu, Dian and Chen, Jianshu and Yu, Dong and Choi, Yejin and Cardie, Claire},
	year = {2019},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {217--231},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/MJ484SRA/Sun et al. - 2019 - DREAM A Challenge Data Set and Models for Dialogu.pdf:application/pdf},
}

@inproceedings{welbl_crowdsourcing_2017,
	title = {Crowdsourcing {Multiple} {Choice} {Science} {Questions}},
	doi = {10.18653/v1/W17-4413},
	abstract = {This work presents a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers by leveraging a large corpus of domain-specific text and a small set of existing questions and shows that humans cannot distinguish the crowdsourced questions from original questions. We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions. We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.},
	booktitle = {{NUT}@{EMNLP}},
	author = {Welbl, Johannes and Liu, Nelson F. and Gardner, Matt},
	year = {2017},
	file = {Full Text:/Users/domenicrosati/Zotero/storage/73GLN7DK/Welbl et al. - 2017 - Crowdsourcing Multiple Choice Science Questions.pdf:application/pdf},
}

@article{khot_qasc_2020,
	title = {{QASC}: {A} {Dataset} for {Question} {Answering} via {Sentence} {Composition}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{QASC}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6319},
	doi = {10.1609/aaai.v34i05.6319},
	abstract = {Composing knowledge from multiple pieces of texts is a key challenge in multi-hop question answering. We present a multi-hop reasoning dataset, Question Answering via Sentence Composition (QASC), that requires retrieving facts from a large corpus and composing them to answer a multiple-choice question. QASC is the first dataset to offer two desirable properties: (a) the facts to be composed are annotated in a large corpus, and (b) the decomposition into these facts is not evident from the question itself. The latter makes retrieval challenging as the system must introduce new concepts or relations in order to discover potential decompositions. Further, the reasoning model must then learn to identify valid compositions of these retrieved facts using common-sense reasoning. To help address these challenges, we provide annotation for supporting facts as well as their composition. Guided by these annotations, we present a two-step approach to mitigate the retrieval challenges. We use other multiple-choice datasets as additional training data to strengthen the reasoning model. Our proposed approach improves over current state-of-the-art language models by 11\% (absolute). The reasoning and retrieval problems, however, remain unsolved as this model still lags by 20\% behind human performance.},
	language = {en},
	number = {05},
	urldate = {2022-06-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Khot, Tushar and Clark, Peter and Guerquin, Michal and Jansen, Peter and Sabharwal, Ashish},
	month = apr,
	year = {2020},
	note = {Number: 05},
	pages = {8082--8090},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/X4TEPGW2/Khot et al. - 2020 - QASC A Dataset for Question Answering via Sentenc.pdf:application/pdf},
}

@inproceedings{liang_new_2019,
	title = {A {New} {Multi}-choice {Reading} {Comprehension} {Dataset} for {Curriculum} {Learning}},
	url = {https://proceedings.mlr.press/v101/liang19a.html},
	abstract = {The past few years have witnessed the rapid development of machine reading comprehension (MRC), especially the challenging sub-task, multiple-choice reading comprehension (MCRC). And the release of large scale datasets promotes the research in this field. Yet previous methods have already achieved high accuracy of the MCRC datasets, {\textbackslash}textit\{e.g.\} RACE. It’s necessary to propose a more difficult dataset which needs more reasoning and inference for evaluating the understanding capability of new methods. To respond to such demand, we present RACE-C, a new multi-choice reading comprehension dataset collected from college English examinations in China. And further we integrate it with RACE-M and RACE-H, collected by \{\{Lai et al.\}\} (\{2017\}) from middle and high school exams respectively, to extend RACE to be RACE++. Based on RACE++, we propose a three-stage curriculum learning framework, which is able to use the best of the characteristic that the difficulty level within these three sub-datasets is in ascending order. Statistics show the higher difficulty level of our collected dataset, RACE-C, compared to RACE’s two sub-datasets, {\textbackslash}textit\{i.e.\}, RACE-M and RACE-H. And experimental results demonstrate that our proposed three-stage curriculum learning approach improves the performance of the machine reading comprehension model to an extent.},
	language = {en},
	urldate = {2022-06-18},
	booktitle = {Proceedings of {The} {Eleventh} {Asian} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liang, Yichan and Li, Jianheng and Yin, Jian},
	month = oct,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {742--757},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/5JFXGDXU/Liang et al. - 2019 - A New Multi-choice Reading Comprehension Dataset f.pdf:application/pdf},
}

@inproceedings{ostermann_mcscript_2018,
	address = {Miyazaki, Japan},
	title = {{MCScript}: {A} {Novel} {Dataset} for {Assessing} {Machine} {Comprehension} {Using} {Script} {Knowledge}},
	shorttitle = {{MCScript}},
	url = {https://aclanthology.org/L18-1564},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC} 2018)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Ostermann, Simon and Modi, Ashutosh and Roth, Michael and Thater, Stefan and Pinkal, Manfred},
	month = may,
	year = {2018},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/9MII86TB/Ostermann et al. - 2018 - MCScript A Novel Dataset for Assessing Machine Co.pdf:application/pdf},
}

@inproceedings{ostermann_mcscript20_2019,
	address = {Minneapolis, Minnesota},
	title = {{MCScript2}.0: {A} {Machine} {Comprehension} {Corpus} {Focused} on {Script} {Events} and {Participants}},
	shorttitle = {{MCScript2}.0},
	url = {https://aclanthology.org/S19-1012},
	doi = {10.18653/v1/S19-1012},
	abstract = {We introduce MCScript2.0, a machine comprehension corpus for the end-to-end evaluation of script knowledge. MCScript2.0 contains approx. 20,000 questions on approx. 3,500 texts, crowdsourced based on a new collection process that results in challenging questions. Half of the questions cannot be answered from the reading texts, but require the use of commonsense and, in particular, script knowledge. We give a thorough analysis of our corpus and show that while the task is not challenging to humans, existing machine comprehension models fail to perform well on the data, even if they make use of a commonsense knowledge base. The dataset is available at http://www.sfb1102. uni-saarland.de/?page\_id=2582},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the {Eighth} {Joint} {Conference} on {Lexical} and {Computational} {Semantics} (*{SEM} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Ostermann, Simon and Roth, Michael and Pinkal, Manfred},
	month = jun,
	year = {2019},
	pages = {103--117},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/I85HSG5X/Ostermann et al. - 2019 - MCScript2.0 A Machine Comprehension Corpus Focuse.pdf:application/pdf},
}

@inproceedings{richardson_mctest_2013,
	address = {Seattle, Washington, USA},
	title = {{MCTest}: {A} {Challenge} {Dataset} for the {Open}-{Domain} {Machine} {Comprehension} of {Text}},
	shorttitle = {{MCTest}},
	url = {https://aclanthology.org/D13-1020},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Richardson, Matthew and Burges, Christopher J.C. and Renshaw, Erin},
	month = oct,
	year = {2013},
	pages = {193--203},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/JXCEGREN/Richardson et al. - 2013 - MCTest A Challenge Dataset for the Open-Domain Ma.pdf:application/pdf},
}

@inproceedings{lai_race_2017,
	address = {Copenhagen, Denmark},
	title = {{RACE}: {Large}-scale {ReAding} {Comprehension} {Dataset} {From} {Examinations}},
	shorttitle = {{RACE}},
	url = {https://aclanthology.org/D17-1082},
	doi = {10.18653/v1/D17-1082},
	abstract = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/ glai1/data/race/and the code is available at https://github.com/qizhex/RACE\_AR\_baselines.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
	month = sep,
	year = {2017},
	pages = {785--794},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/NPNQXCV9/Lai et al. - 2017 - RACE Large-scale ReAding Comprehension Dataset Fr.pdf:application/pdf},
}

@misc{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s {Transformers}: {State}-of-the-art {Natural} {Language} {Processing}},
	shorttitle = {{HuggingFace}'s {Transformers}},
	url = {http://arxiv.org/abs/1910.03771},
	doi = {10.48550/arXiv.1910.03771},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	month = jul,
	year = {2020},
	note = {Number: arXiv:1910.03771
arXiv:1910.03771 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/F2Y9K5K8/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/443SVEQG/1910.html:text/html},
}

@inproceedings{harabagiu_methods_2006,
	address = {Sydney, Australia},
	title = {Methods for {Using} {Textual} {Entailment} in {Open}-{Domain} {Question} {Answering}},
	url = {https://aclanthology.org/P06-1114},
	doi = {10.3115/1220175.1220289},
	urldate = {2022-06-19},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Computational} {Linguistics} and 44th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Harabagiu, Sanda and Hickl, Andrew},
	month = jul,
	year = {2006},
	pages = {905--912},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/4B9V592Y/Harabagiu and Hickl - 2006 - Methods for Using Textual Entailment in Open-Domai.pdf:application/pdf},
}

@inproceedings{gardner_evaluating_2020,
	address = {Online},
	title = {Evaluating {Models}' {Local} {Decision} {Boundaries} via {Contrast} {Sets}},
	url = {https://aclanthology.org/2020.findings-emnlp.117},
	doi = {10.18653/v1/2020.findings-emnlp.117},
	abstract = {Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets—up to 25\% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.},
	urldate = {2022-06-19},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Gardner, Matt and Artzi, Yoav and Basmov, Victoria and Berant, Jonathan and Bogin, Ben and Chen, Sihao and Dasigi, Pradeep and Dua, Dheeru and Elazar, Yanai and Gottumukkala, Ananth and Gupta, Nitish and Hajishirzi, Hannaneh and Ilharco, Gabriel and Khashabi, Daniel and Lin, Kevin and Liu, Jiangming and Liu, Nelson F. and Mulcaire, Phoebe and Ning, Qiang and Singh, Sameer and Smith, Noah A. and Subramanian, Sanjay and Tsarfaty, Reut and Wallace, Eric and Zhang, Ally and Zhou, Ben},
	month = nov,
	year = {2020},
	pages = {1307--1323},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/JFVFR86C/Gardner et al. - 2020 - Evaluating Models' Local Decision Boundaries via C.pdf:application/pdf},
}

@inproceedings{bostrom_flexible_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Flexible {Generation} of {Natural} {Language} {Deductions}},
	url = {https://aclanthology.org/2021.emnlp-main.506},
	doi = {10.18653/v1/2021.emnlp-main.506},
	abstract = {An interpretable system for open-domain reasoning needs to express its reasoning process in a transparent form. Natural language is an attractive representation for this purpose — it is both highly expressive and easy for humans to understand. However, manipulating natural language statements in logically consistent ways is hard: models must cope with variation in how meaning is expressed while remaining precise. In this paper, we describe ParaPattern, a method for building models to generate deductive inferences from diverse natural language inputs without direct human supervision. We train BART-based models (Lewis et al., 2020) to generate the result of applying a particular logical operation to one or more premise statements. Crucially, we develop a largely automated pipeline for constructing suitable training examples from Wikipedia. We evaluate our models using out-of-domain sentence compositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et al., 2021) datasets as well as targeted perturbation sets. Our results show that our models are substantially more accurate and flexible than baseline systems. ParaPattern achieves 85\% validity on examples of the `substitution' operation from EntailmentBank without the use of any in-domain training data, matching the performance of a model fine-tuned for EntailmentBank. The full source code for our method is publicly available.},
	urldate = {2022-06-19},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bostrom, Kaj and Zhao, Xinyu and Chaudhuri, Swarat and Durrett, Greg},
	month = nov,
	year = {2021},
	pages = {6266--6278},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/F6QKES3C/Bostrom et al. - 2021 - Flexible Generation of Natural Language Deductions.pdf:application/pdf},
}
